# üìò Guide de R√©f√©rence : Int√©gration IA G√©n√©rative (RAG)

Ce document sert de **r√©f√©rence technique** pour int√©grer un mod√®le d'IA (comme Google Gemini ou OpenAI) dans une application web existante ou future (ex: PharmaGN). Il d√©taille l'architecture RAG mise en place dans EcoWatch.

---

## 1. üéØ Objectif & Concept

L'objectif est de permettre √† l'IA de r√©pondre √† des questions sur des donn√©es **priv√©es et temps r√©el** (capteurs, stocks, patients...), ce qu'elle ne peut pas faire nativement.

### Architecture RAG Simplifi√©e (Retrieval-Augmented Generation)

1.  **Retrieval (R√©cup√©ration)** : Le syst√®me va chercher les donn√©es fra√Æches en Base de Donn√©es.
2.  **Augmented (Enrichissement)** : Le syst√®me injecte ces donn√©es dans le "cerveau" de l'IA via le prompt.
3.  **Generation (R√©ponse)** : L'IA formule une r√©ponse naturelle bas√©e *uniquement* sur ces donn√©es inject√©es.

| Avantage | Pourquoi c'est crucial ? |
| :--- | :--- |
| **Z√©ro Hallucination** | L'IA ne peut pas inventer de chiffres, elle lit ce qu'on lui donne. |
| **Temps R√©el** | Pas besoin de r√©entra√Æner le mod√®le √† chaque nouvelle donn√©e. |
| **S√©curit√©** | Les donn√©es ne servent qu'au contexte de la conversation, elles ne partent pas entra√Æner le mod√®le public (selon les CGU Pro). |

---

## 2. üîÑ Le Flux de Donn√©es (Data Flow)

Voici le parcours exact d'une question, du clic utilisateur jusqu'√† l'affichage de la r√©ponse.

```mermaid
sequenceDiagram
    participant User as üë§ Utilisateur
    participant Front as üñ•Ô∏è Frontend (HTMX)
    participant View as ‚öôÔ∏è Django View
    participant DB as üóÑÔ∏è Base de Donn√©es
    participant AI as üß† Service IA
    participant Google as ‚òÅÔ∏è API Gemini

    User->>Front: Pose une question ("Qualit√© de l'air ?")
    Front->>View: POST /chatbot-query (Async)
    
    rect rgb(240, 248, 255)
    Note over View, DB: √âtape RAG : Retrieval
    View->>DB: R√©cup√®re les mesures r√©centes (Top 5)
    DB-->>View: Liste d'objets [Reading 1, Reading 2...]
    View->>View: Convertit les objets en Texte (S√©rialisation)
    end
    
    View->>AI: Appelle get_chat_response(question, contexte)
    
    rect rgb(255, 240, 245)
    Note over AI, Google: √âtape RAG : Generation
    AI->>Google: Envoie PROMPT SYSTEM + CONTEXTE + QUESTION
    Google-->>AI: Renvoie la r√©ponse g√©n√©r√©e (Markdown)
    end
    
    AI-->>View: Texte de la r√©ponse
    View-->>Front: HTML partiel (Bubble Message)
    Front-->>User: Affiche le message (Rendu Markdown)
```

---

## 3. üõ†Ô∏è Impl√©mentation Technique (Code & R√¥les)

### 3.1. Le Cerveau (Service IA)
**Fichier :** [`monitoring/services/gemini/service.py`](file:///d:/Projet_Python/groupe4/monitoring/services/gemini/service.py)
C'est ici qu'on "programme" le comportement de l'IA via le **System Prompt**.

```python
# Le prompt syst√®me d√©finit la personnalit√© et les r√®gles
system_prompt = f"""
Tu es l'assistant EcoWatch.
Voici les donn√©es actuelles (CONTEXTE R√âEL) :
{context_data}

Consigne : R√©ponds √† la question de l'utilisateur en utilisant UNIQUEMENT le contexte ci-dessus.
"""
```

### 3.2. Le Chef d'Orchestre (Vue Django)
**Fichier :** [`monitoring/views_htmx.py`](file:///d:/Projet_Python/groupe4/monitoring/views_htmx.py)
C'est lui qui fait le lien entre vos donn√©es et l'IA.

```python
def chatbot_query(request):
    # 1. R√âCUP√âRATION (Retrieval)
    # On prend les 5 derniers relev√©s pour ne pas saturer le contexte
    readings = Reading.objects.order_by('-timestamp')[:5]
    
    # 2. S√âRIALISATION
    # On transforme les objets DB en texte lisible par l'IA
    context_str = "\n".join([f"{r.timestamp}: {r.value}" for r in readings])
    
    # 3. G√âN√âRATION
    response = ai_service.get_response(user_msg, context=context_str)
    
    return render(request, 'partials/message.html', {'response': response})
```

### 3.3. L'Interface (Frontend)
**Fichier :** [`monitoring/templates/monitoring/partials/chatbot.html`](file:///d:/Projet_Python/groupe4/monitoring/templates/monitoring/partials/chatbot.html)
- Utilise **HTMX** pour l'envoi sans rechargement (`hx-post`).
- Utilise **Marked.js** pour transformer le Markdown de l'IA (gras, listes) en HTML propre.

---

## 4. ‚úÖ Bonnes Pratiques pour Futurs Projets (ex: PharmaGN)

1.  **Toujours Contextualiser (RAG)**
    - Ne demandez jamais "Quel est le stock ?" √† l'IA brute.
    - Donnez-lui : "Le stock est : Doliprane (50), Advil (10). Quel est le stock ?"

2.  **S√©parer les Services**
    - Gardez une couche d'abstraction (`AIService`). Si demain Google augmente ses prix, vous pourrez passer √† OpenAI ou Claude en changeant une seule ligne dans [`monitoring/services/gemini/config.py`](file:///d:/Projet_Python/groupe4/monitoring/services/gemini/config.py).

3.  **G√©rer les Erreurs**
    - Les API d'IA peuvent √©chouer (timeout, quota). Pr√©voyez toujours un `try/except` qui renvoie "Service indisponible" plut√¥t que de faire planter la page.

4.  **S√©curit√© des Cl√©s (Critique)**
    - Jamais de cl√© API dans le code (`.py`).
    - Toujours dans un fichier `.env` non-versionn√©.
    - Utilisez `.gitignore` pour exclure `.env`.

5.  **Optimiser les Co√ªts**
    - N'envoyez pas *toute* la base de donn√©es dans le contexte. Filtrez les donn√©es pertinentes (ex: "Les 5 derni√®res commandes" ou "Les produits dont le stock < 10").

---

## 5. D√©pannage Rapide

- **R√©ponse "Je ne sais pas"** : V√©rifiez que `context_data` n'est pas vide dans la vue.
- **Erreur 429 (Quota)** : Vous avez d√©pass√© la limite gratuite. Attendez quelques minutes.
- **KeyError/Auth** : V√©rifiez que `GOOGLE_GENAI_API_KEY` est bien charg√©e dans `os.environ`.